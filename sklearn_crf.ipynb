{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "#created time:2017-12-12 14:39:00\n",
    "#Author:kyla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangwenhui/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/wangwenhui/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')\n",
    "from itertools import chain\n",
    "\n",
    "# import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import codecs\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace every digit in a string by a zero.\n",
    "    \"\"\"\n",
    "    s[0] = re.sub('\\d', '0', s[0])\n",
    "    return s\n",
    "\n",
    "\n",
    "def load_sentences(path, lower, zeros):\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        if not line.rstrip():\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = zero_digits(line.rstrip().split()) if zeros else line.rstrip().split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.isalpha()': word.isalpha(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.isalpha()': word1.isalpha(),\n",
    "            '-1:word.isdigit()': word1.isdigit(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.isalpha()': word1.isalpha(),\n",
    "            '+1:word.isdigit()': word1.isdigit(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.11 s, sys: 129 ms, total: 2.24 s\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lower = 1\n",
    "zeros = 0\n",
    "train_path = './train.txt'\n",
    "train_sents = load_sentences(train_path, lower, zeros)\n",
    "\n",
    "dev_path = './dev.txt'\n",
    "dev_sents = load_sentences(dev_path, lower, zeros)\n",
    "\n",
    "test_path = './test.txt'\n",
    "test_sents = load_sentences(test_path, lower, zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.54 s, sys: 387 ms, total: 2.93 s\n",
      "Wall time: 3.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_dev = [sent2features(s) for s in dev_sents]\n",
    "y_dev = [sent2labels(s) for s in dev_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.5 s, sys: 775 ms, total: 1min\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(test_sents, y_test, y_pred):\n",
    "    predictions = []\n",
    "    n_tags = len(np.unique([x for ys in y_pred for x in ys]))\n",
    "    count = np.zeros((n_tags, n_tags), dtype=np.int32)\n",
    "    tag_to_id = {\"B-ORG\": 0, \"I-ORG\": 1, \"E-ORG\": 2, \"S-ORG\": 3, \"O\": 4}\n",
    "    id_to_tag = {0: \"B-ORG\", 1: \"I-ORG\", 2: \"E-ORG\", 3: \"S-ORG\", 4: \"O\"}\n",
    "    \n",
    "    for raw_sentence, y_reals, y_preds in zip(test_sents, y_test, y_pred):\n",
    "#         print len(y_preds)\n",
    "#         print len(y_reals)\n",
    "        assert len(y_preds) == len(y_reals)\n",
    "        for i, (yr, yp) in enumerate(zip(y_reals, y_preds)):\n",
    "            new_line = \" \".join(raw_sentence[i][:-1] + [yr, yp])\n",
    "            predictions.append(new_line)\n",
    "            count[tag_to_id[yr], tag_to_id[yp]] += 1\n",
    "        predictions.append(\"\")\n",
    "    \n",
    "    # Write predictions to disk and run CoNLL script externally\n",
    "    eval_script = './evaluation/conlleval'\n",
    "    eval_temp = './evaluation/temp'\n",
    "    eval_id = np.random.randint(1000000, 2000000)\n",
    "    print \"eval_id: %d\" % (eval_id)\n",
    "    output_path = os.path.join(eval_temp, \"eval.%i.output\" % eval_id)\n",
    "    scores_path = os.path.join(eval_temp, \"eval.%i.scores\" % eval_id)\n",
    "    with codecs.open(output_path, 'w', 'utf8') as f:\n",
    "        f.write(\"\\n\".join(predictions))\n",
    "    os.system(\"perl %s < %s > %s\" % (eval_script, output_path, scores_path))\n",
    "\n",
    "    # CoNLL evaluation results\n",
    "    eval_lines = [l.rstrip() for l in codecs.open(scores_path, 'r', 'utf8')]\n",
    "    print \"eval_lines:\"\n",
    "    for line in eval_lines:\n",
    "        print line\n",
    "\n",
    "    # Remove temp files\n",
    "    #os.remove(output_path)\n",
    "    #os.remove(scores_path)\n",
    "\n",
    "    # Confusion matrix with accuracy for each tag\n",
    "    print (\"{: >2}{: >7}{: >7}%s{: >9}\" % (\"{: >7}\" * n_tags)).format(\n",
    "        \"ID\", \"NE\", \"Total\",\n",
    "        *([id_to_tag[i] for i in xrange(n_tags)] + [\"Percent\"])\n",
    "    )\n",
    "    for i in xrange(n_tags):\n",
    "        print (\"{: >2}{: >7}{: >7}%s{: >9}\" % (\"{: >7}\" * n_tags)).format(\n",
    "            str(i), id_to_tag[i], str(count[i].sum()),\n",
    "            *([count[i][j] for j in xrange(n_tags)] +\n",
    "              [\"%.3f\" % (count[i][i] * 100. / max(1, count[i].sum()))])\n",
    "        )\n",
    "\n",
    "    # Global accuracy\n",
    "    print \"%i/%i (%.5f%%)\" % (\n",
    "        count.trace(), count.sum(), 100. * count.trace() / max(1, count.sum())\n",
    "    )\n",
    "\n",
    "    # F1 on all entities\n",
    "    return float(eval_lines[1].strip().split()[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_id: 1263949\n",
      "eval_lines:\n",
      "processed 63241 tokens with 4554 phrases; found: 4166 phrases; correct: 3920.\n",
      "accuracy:  97.53%; precision:  94.10%; recall:  86.08%; FB1:  89.91\n",
      "              ORG: precision:  94.10%; recall:  86.08%; FB1:  89.91  4166\n",
      "ID     NE  Total  B-ORG  I-ORG  E-ORG  S-ORG      O  Percent\n",
      " 0  B-ORG   2825   2457     13      0     22    333   86.973\n",
      " 1  I-ORG   1346     33   1065     19      1    228   79.123\n",
      " 2  E-ORG   2833      0     25   2463      3    342   86.940\n",
      " 3  S-ORG   1842     36      2      5   1573    226   85.396\n",
      " 4      O  54395     75     57    118     23  54122   99.498\n",
      "61680/63241 (97.53166%)\n",
      "CPU times: user 1.28 s, sys: 114 ms, total: 1.4 s\n",
      "Wall time: 2.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dev\n",
    "y_pred = crf.predict(X_dev)\n",
    "evaluate(dev_sents, y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_id: 1915435\n",
      "eval_lines:\n",
      "processed 68798 tokens with 4926 phrases; found: 4502 phrases; correct: 4266.\n",
      "accuracy:  97.61%; precision:  94.76%; recall:  86.60%; FB1:  90.50\n",
      "              ORG: precision:  94.76%; recall:  86.60%; FB1:  90.50  4502\n",
      "ID     NE  Total  B-ORG  I-ORG  E-ORG  S-ORG      O  Percent\n",
      " 0  B-ORG   3078   2686     15      5     27    345   87.264\n",
      " 1  I-ORG   1379     20   1090     21      1    247   79.043\n",
      " 2  E-ORG   3086      1     33   2672      1    379   86.585\n",
      " 3  S-ORG   1978     27      0      7   1697    247   85.794\n",
      " 4      O  59277     74     68    105     23  59007   99.545\n",
      "67152/68798 (97.60749%)\n",
      "CPU times: user 1.56 s, sys: 127 ms, total: 1.69 s\n",
      "Wall time: 3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test\n",
    "y_pred = crf.predict(X_test)\n",
    "evaluate(test_sents, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
